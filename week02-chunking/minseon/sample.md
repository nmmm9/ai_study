# RAG (Retrieval-Augmented Generation) 개요

## RAG란 무엇인가?

RAG는 대규모 언어 모델(LLM)의 한계를 보완하기 위해 등장한 기술이다. LLM은 학습 데이터에 포함되지 않은 최신 정보나 도메인 특화 지식에 대해 정확한 답변을 제공하기 어렵다. RAG는 외부 지식 베이스에서 관련 문서를 검색(Retrieval)한 뒤, 이를 LLM의 입력에 추가(Augmented)하여 더 정확한 답변을 생성(Generation)하는 방식이다.

기존 LLM만 사용하는 경우 발생하는 주요 문제점은 다음과 같다. 첫째, 학습 데이터의 시점 이후 정보를 알 수 없는 지식 단절 문제가 있다. 둘째, 특정 도메인의 전문 지식이 부족할 수 있다. 셋째, 사실이 아닌 내용을 그럴듯하게 생성하는 할루시네이션 문제가 있다.

## RAG의 핵심 구성 요소

### 1. 문서 로딩 (Document Loading)

다양한 형식의 문서를 시스템에 로드하는 단계이다. PDF, Markdown, HTML, CSV 등 다양한 포맷을 지원해야 한다. LangChain에서는 `DocumentLoader` 클래스를 통해 다양한 형식의 문서를 통일된 인터페이스로 로드할 수 있다.

### 2. 청킹 (Chunking)

로드된 문서를 적절한 크기의 조각으로 분할하는 과정이다. 청킹이 중요한 이유는 임베딩 모델의 입력 길이 제한과 검색 정확도에 직접적인 영향을 미치기 때문이다. 너무 큰 청크는 검색 정확도를 떨어뜨리고, 너무 작은 청크는 문맥 정보를 잃게 된다.

주요 청킹 전략으로는 고정 크기 분할, 재귀적 문자 분할, 의미 기반 분할이 있다. 재귀적 문자 분할은 문단, 줄바꿈, 문장 부호 순서로 우선순위를 두고 분할하여 의미 단위를 최대한 보존한다.

### 3. 임베딩 (Embedding)

텍스트 청크를 벡터 공간의 숫자 배열로 변환하는 과정이다. 의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치된다. OpenAI의 text-embedding-ada-002나 text-embedding-3-small 등의 모델이 널리 사용된다.

### 4. 벡터 저장소 (Vector Store)

임베딩된 벡터를 저장하고 유사도 검색을 수행하는 데이터베이스이다. Chroma, Pinecone, Weaviate, FAISS 등 다양한 벡터 데이터베이스가 있다. 각각 로컬 실행, 클라우드 호스팅, 확장성 등에서 차이가 있다.

### 5. 검색 및 생성 (Retrieval & Generation)

사용자 질문을 임베딩한 뒤 벡터 저장소에서 유사한 문서 청크를 검색한다. 검색된 청크를 LLM의 프롬프트에 컨텍스트로 포함시켜 답변을 생성한다. 이 과정에서 적절한 프롬프트 엔지니어링이 중요하다.

## RAG의 장점과 한계

### 장점

RAG를 사용하면 LLM을 재학습하지 않고도 최신 정보를 반영할 수 있다. 또한 답변의 출처를 명확히 제시할 수 있어 신뢰성이 높아진다. 도메인 특화 지식을 쉽게 추가할 수 있으며, 할루시네이션을 줄일 수 있다.

### 한계

검색 품질에 따라 전체 시스템 성능이 좌우된다. 청킹 전략이 부적절하면 관련 정보를 찾지 못할 수 있다. 또한 벡터 데이터베이스의 운영과 관리에 추가적인 인프라가 필요하다. 대규모 문서 처리 시 임베딩 비용도 고려해야 한다.
