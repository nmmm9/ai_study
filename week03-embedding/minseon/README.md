# 3주차: Embedding & Vector DB - minseon

## 기술 스택
| 항목 | 선택 | 대안 | 선택 이유 |
|------|------|------|----------|
|임베딩 모델|OpenAI text-embedding-3-small|text-embedding-ada-002, HuggingFace| 1주차부터 OpenAI를 써왔으니 일관성 유지. ada-002보다 성능은 높고 비용은 더 저렴함 |
|Vector DB|ChromaDB|Pinecone, FAISS, Weaviate|서버 설치 없이 로컬에서 바로 실행 가능. 설정이 가장 단순함|
|유사도 측정|코사인 유사도|유클리드 거리, 내적|	벡터의 크기가 아닌 방향(의미)만 비교해서 텍스트 검색에 더 적합함|

## 핵심 구현
- 주요 로직 설명: 
1. 문서 로딩: 파일 확장자 자동 감지 (.md / .txt / .pdf) 후 텍스트 추출
2. 임베딩 : OpenAI text-embedding-3-small으로 청크 전체를 한 번에 배치 처리 -> 각 청크를 1536차원 숫자 벡터로 변환
3. 저장 : ChromaDB에 벡터 + 원문 텍스트 + 메타데이터(파일명, 청크 번호) 함께 로컬 저장
4. 검색 : 질문을 임베딩 후 코사인 유사도로 가장 의미가 가까운 청크 top-k개 반환

사용자가 질문을 던지면 -> 질문을 숫자로 바꾸고 -> 저장된 수많은 문서 조각들과 의미가 얼마나 비슷한지 각도(코사인 유사도) 재서 -> 가장 정답에 가까운 핵심 문서 조각 3~5개(top-k)만 가져오기

- 코드 실행 방법:
# 패키지 설치
pip install -r requirements.txt

# .env 파일에 OpenAI API 키 설정
# OPENAI_API_KEY=sk-...

# 1단계: 문서 임베딩 후 DB 저장
python embedder.py index sample.md

# 2단계: 질문으로 유사한 청크 검색
python embedder.py search "RAG란 무엇인가?"

# 검색 결과 수 조정 (기본값: 3)
python embedder.py search "청킹이 중요한 이유는?" --top-k 5


## WHY (의사결정 기록)
1. **Q**: 왜 이 방식을 선택했는가?
   **A**: 임베딩 모델로 text-embedding-3-small을 선택한 이유는 OpenAI 모델 중 비용 대비 성능이 가장 균형 잡혀 있기 때문이다.
벡터 저장소로는 ChromaDB했는데 실행하다가 오류가나서 numpy + JSON 방식으로 변경하였다. ChromaDB가 Python 3.14와 호환되지 않는 문제가 발생했다. (실제 서비스라면 대규모 문서에 최적화된 ChromaDB나 FAISS를 사용하는게 적합함)

청킹 전략은 2주차와 동일 (RecursiveCharacterTextSplitter 사용)

답변 생성에 gpt-4o를 선택한 이유는 RAG 파이프라인에서 검색된 컨텍스트를 정확히 해석하고 한국어로 자연스럽게 답변하는 능력이 중요하기 때문이다. 
2. **Q**: 다르게 구현한다면 어떻게 했을까?
   **A**: 벡터 저장소로 ChromaDB를 사용했을것이다

## 트러블슈팅 로그
| # | 문제 상황 | 에러 메시지 | 원인 (Root Cause) | 해결 방법 |
|---|----------|-----------|-------------------|----------|
| 1 | | | | |

## 회고
- 이번 주 배운 점: Python 3.14와 ChromaDB의 호환 문제가 있다는 것을 알게되었다. 최신 Python 버전이 항상 모든 라이브러리와 호환되지는 않는다는 것을 알게되었다 
- 다음 주 준비할 것: -rip pipeline 알아보기 





















임베딩(Embedding) : 우리가 쓰는 자연어(글자, 문장)를 컴퓨터가 이해할 수 있는 숫자들의 배열(벡터)로 변환해 주는 기술. 의미가 비슷한 문장들은 좌표상에서 서로 가까운 곳에 위치함 그래서 검색이나 추천, 분류 작업을 수행가능.


Vector DB ? 두 벡터 사이의 거리를 계산하여 유사도 측정.
1. 지식의 외부 저장소 : LLM(ChatGPT 등)은 학습된 데이터만 압축해서 알고 있음

2. 검색 후 답변 : 사용자가 질문을 하면 벡터 DB에서 관련 문서를 빠르게 찾아낸 뒤 그 내용을 LLM에게 전달하여 답변하게 만듦.

3. 환각 방지: 최신 정보나 내부 기밀 데이터를 벡터 DB에 넣어두면 AI가 엉뚱한 소리를 하지 않고 근거 있는 답변을 함

ChromaDB ? 
- 서버 설치 없이 로컬 환경에서 바로 실행 가능
Pinecone과 같은 클라우드 기반 서비스나 별도의 도커/서버 세팅이 필요한 DB들과 다르게 ChromaDB는 파이썬 패키지(pip install chromadb)만 설치하면 로컬 환경에서 바로 사용할 수 있음.
- 학습 및 프로토타이핑에 가장 단순한 설정
ChromaDB는 직관적이고 단순한 구조라서 초보자도 쉽고 빠르게 벡터 데이터를 저장하고 유사도 검색을 수행할 수 있음. 


유사도 측정
1. 유클리드 거리(Euclidean Distance) : 두 점 사이의 직선거리. 같은 주제를 다루더라도 텍스트의 길이가 길어지면(단어 빈도가 높아지면) 벡터의 크기가 커져 거리가 멀다고 잘못 판단할 수도 있음. 
2. 내적(Dot Product) 등 방법은 더 있음 
3. (텍스트 검색에는 주로) 코사인 유사도(Cosine Similarity) : 두 벡터가 이루는 각도(방향)만 측정. 벡터의 길이(텍스트의 분량)와 상관없이 두 문장이 같은 곳(주제, 의미)을 가리키고 있는지를 파악하는 데 특화되어 있습니다.