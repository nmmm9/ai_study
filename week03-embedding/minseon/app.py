"""
3ì£¼ì°¨ ê³¼ì œ: Embedding & Vector DB - Streamlit RAG ì±—ë´‡
ë¬¸ì„œë¥¼ ì„ë² ë”©í•´ì„œ ChromaDBì— ì €ì¥ í›„, ì§ˆë¬¸ì— ê´€ë ¨ ì²­í¬ë¥¼ ê²€ìƒ‰í•´ GPTë¡œ ë‹µë³€
"""

import os
import streamlit as st
from dotenv import load_dotenv

from embedder import embed_texts, get_collection, store_embeddings, search, split_text, load_document

load_dotenv()

# â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="RAG ì±—ë´‡", page_icon="ğŸ“š")
st.title("ğŸ“š RAG ì±—ë´‡ (3ì£¼ì°¨)")
st.caption("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.")

# â”€â”€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "indexed" not in st.session_state:
    st.session_state.indexed = False

# â”€â”€ ì‚¬ì´ë“œë°”: ë¬¸ì„œ ì—…ë¡œë“œ & ì¸ë±ì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("ë¬¸ì„œ ì„¤ì •")

    uploaded = st.file_uploader("ë¬¸ì„œ ì—…ë¡œë“œ (md / txt / pdf)", type=["md", "txt", "pdf"])

    if uploaded and st.button("ì„ë² ë”© & ì €ì¥", type="primary"):
        # ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œ ì €ì¥
        tmp_path = f"./tmp_{uploaded.name}"
        with open(tmp_path, "wb") as f:
            f.write(uploaded.read())

        with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
            text = load_document(tmp_path)
            chunks = split_text(text)
            vectors = embed_texts(chunks)
            collection = get_collection()
            store_embeddings(collection, chunks, vectors, uploaded.name)

        os.remove(tmp_path)
        st.session_state.indexed = True
        st.success(f"ì™„ë£Œ! {len(chunks)}ê°œ ì²­í¬ ì €ì¥ë¨")

    st.divider()

    top_k = st.slider("ê²€ìƒ‰í•  ì²­í¬ ìˆ˜ (top-k)", min_value=1, max_value=5, value=3)
    show_chunks = st.checkbox("ì°¸ì¡° ì²­í¬ ë³´ê¸°", value=True)

    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# â”€â”€ ìƒíƒœ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not st.session_state.indexed:
    st.info("ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'ì„ë² ë”© & ì €ì¥' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# â”€â”€ ì´ì „ ëŒ€í™” ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])
        if msg.get("chunks") and show_chunks:
            with st.expander("ì°¸ì¡°í•œ ë¬¸ì„œ ì²­í¬"):
                for i, chunk in enumerate(msg["chunks"]):
                    st.markdown(f"**[{i+1}] ìœ ì‚¬ë„: {chunk['similarity']:.3f}**")
                    st.text(chunk["content"][:300] + ("..." if len(chunk["content"]) > 300 else ""))

# â”€â”€ ì‚¬ìš©ì ì…ë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", disabled=not st.session_state.indexed):

    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.write(user_input)

    with st.chat_message("assistant"):
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            collection = get_collection()
            hits = search(collection, user_input, top_k=top_k)

        # ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        context = "\n\n---\n\n".join([h["content"] for h in hits])
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{user_input}

ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”."""

        from openai import OpenAI
        client = OpenAI()

        stream = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AIì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt},
            ],
            stream=True,
        )

        full_response = st.write_stream(
            chunk.choices[0].delta.content
            for chunk in stream
            if chunk.choices and chunk.choices[0].delta.content
        )

        # ì°¸ì¡° ì²­í¬ í‘œì‹œ
        chunks_info = [{"content": h["content"], "similarity": 1 - h["distance"]} for h in hits]
        if show_chunks:
            with st.expander("ì°¸ì¡°í•œ ë¬¸ì„œ ì²­í¬"):
                for i, chunk in enumerate(chunks_info):
                    st.markdown(f"**[{i+1}] ìœ ì‚¬ë„: {chunk['similarity']:.3f}**")
                    st.text(chunk["content"][:300] + ("..." if len(chunk["content"]) > 300 else ""))

    st.session_state.messages.append({
        "role": "assistant",
        "content": full_response,
        "chunks": chunks_info,
    })
