"""
1ì£¼ì°¨ ê³¼ì œ: OpenAI GPT-4o - Streamlit ì›¹ ì±—ë´‡
"""

import os
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()
client = OpenAI()

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL = "gpt-4o"
MAX_TOKENS = 1024
SYSTEM_PROMPT = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."

# â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="GPT-4o ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ¤– GPT-4o 1:1 ì±„íŒ…")

# â”€â”€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]
if "total_input_tokens" not in st.session_state:
    st.session_state.total_input_tokens = 0
if "total_output_tokens" not in st.session_state:
    st.session_state.total_output_tokens = 0

# â”€â”€ ì´ˆê¸°í™” ë²„íŠ¼ + í† í° ì‚¬ìš©ëŸ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
with col1:
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        st.session_state.total_input_tokens = 0
        st.session_state.total_output_tokens = 0
        st.rerun()
with col2:
    st.metric("ì…ë ¥ í† í°", st.session_state.total_input_tokens)
with col3:
    st.metric("ì¶œë ¥ í† í°", st.session_state.total_output_tokens)
with col4:
    st.metric("ì´ í† í°", st.session_state.total_input_tokens + st.session_state.total_output_tokens)

# â”€â”€ ì´ì „ ëŒ€í™” ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for msg in st.session_state.messages:
    if msg["role"] == "system":
        continue
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# â”€â”€ ì‚¬ìš©ì ì…ë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):

    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.write(user_input)

    # AI ì‘ë‹µ (Streaming)
    with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model=MODEL,
            max_tokens=MAX_TOKENS,
            messages=st.session_state.messages,
            stream=True,
            stream_options={"include_usage": True},
        )

        def generate():
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                if chunk.usage:
                    st.session_state.total_input_tokens += chunk.usage.prompt_tokens
                    st.session_state.total_output_tokens += chunk.usage.completion_tokens

        full_response = st.write_stream(generate())

    st.session_state.messages.append({"role": "assistant", "content": full_response})
