# 1주차: LLM API 연동 - juwon

## 기술 스택

| 항목        | 선택                      | 대안                          | 선택 이유                             |
| --------- | ----------------------- | --------------------------- | --------------------------------- |
| LLM 호출 방식 | OpenAI Python SDK       | HTTP 직접 호출 (requests, curl) | 구현이 간단하고 Streaming 처리 및 에러 관리가 용이 |
| 서버 구조     | 로컬 터미널 기반 실행            | FastAPI 서버 프록시 구조           | 1주차 목표는 API 이해 및 연동이므로 단순 구조 선택   |
| 모델        | gpt-4o-mini             | gpt-4.1 / gpt-4.1-mini      | 비용 효율적이며 학습 및 테스트에 적합             |
| 키 관리 방식   | .env 환경변수 로드            | 코드 내 하드코딩                   | 보안상 안전하며 실무 표준 방식                 |
| 응답 처리 방식  | Streaming (stream=True) | Non-streaming 응답            | 사용자 경험 개선 및 실시간 출력 구현 목적          |

## 핵심 구현

* 주요 로직 설명:

  * OpenAI Python SDK를 사용하여 GPT 모델과 1:1 대화 기능을 구현하였다.
  * `.env` 파일을 통해 API 키를 안전하게 로드하고, 코드 내에 직접 노출하지 않도록 설계하였다.
  * `stream=True` 옵션을 활용하여 Streaming 응답을 구현하였으며, chunk 단위로 생성되는 텍스트를 실시간으로 출력하였다.
  * 대화 기록을 `conversation` 리스트에 저장하고, Sliding Window 방식을 적용하여 최근 N개의 메시지만 유지하도록 설계하였다.
  * 전체 대화 글자 수 제한(MAX_TOTAL_CHARS)을 두어 토큰 과다 사용을 방지하였다.
  * Streaming 응답에서 usage 정보가 제한되므로, 글자 수 기반 토큰 근사치 계산 방식을 도입하여 입력/출력 토큰을 누적 관리하였다.

* 코드 실행 방법:

  1. 가상환경 생성 (선택)

     ```
     python -m venv venv
     venv\Scripts\activate
     ```
  2. 패키지 설치

     ```
     pip install openai python-dotenv
     ```
  3. 프로젝트 루트에 `.env` 파일 생성

     ```
     OPENAI_API_KEY=sk-본인키입력
     ```
  4. 실행

     ```
     python main.py
     ```
  5. 사용 가능한 명령어

     * `quit` : 프로그램 종료
     * `reset` : 대화 기록 초기화
     * `usage` : 누적 토큰 사용량 확인

## WHY (의사결정 기록)

1. **Q**: 왜 이 방식을 선택했는가?
   **A**:
   1주차의 목표는 LLM API의 기본 구조를 이해하고 직접 연동해보는 것이었기 때문에, 구현 난이도가 낮고 학습 효율이 높은 OpenAI Python SDK 방식을 선택하였다. SDK는 HTTP 요청, 인증 헤더 설정, 응답 파싱 등을 내부적으로 처리해주므로 API 구조 이해에 집중할 수 있다. 또한 Streaming 기능 구현이 간단하여 실시간 응답 처리 방식까지 학습할 수 있다는 점에서 적합하다고 판단하였다.

2. **Q**: 다르게 구현한다면 어떻게 했을까?
   **A**:
   HTTP 직접 호출 방식(requests 라이브러리 활용)으로 REST API를 직접 호출하여 구현할 수 있다. 이 경우 API 요청 구조를 더 깊게 이해할 수 있다는 장점이 있지만, 헤더 설정 및 응답 파싱을 직접 처리해야 하므로 코드 복잡도가 증가한다.
   또한 실무 구조로 확장한다면 FastAPI 서버 프록시 구조를 적용하여 클라이언트 → 서버 → OpenAI 형태로 구현할 수 있다. 이 방식은 API 키 보호와 사용량 통제가 가능하여 서비스 환경에 적합하다.

## 트러블슈팅 로그

| # | 문제 상황           | 에러 메시지           | 원인 (Root Cause)                | 해결 방법                       |
| - | --------------- | ---------------- | ------------------------------ | --------------------------- |
| 1 | API 호출 실패       | 401 Unauthorized | OPENAI_API_KEY 미설정 또는 .env 미로드 | load_dotenv() 확인 및 환경변수 재설정 |
| 2 | 응답이 한 번에 출력됨    | 없음               | stream=True 옵션 누락              | Streaming 옵션 추가             |
| 3 | 대화가 길어질수록 응답 지연 | 없음               | 대화 히스토리 과도 증가                  | Sliding Window 및 글자 수 제한 적용 |

## 회고

* 이번 주 배운 점:

  * OpenAI API 기본 구조 (model, messages, system/user 역할 구조) 이해
  * SDK 방식과 HTTP 방식의 차이점 파악
  * Streaming 응답 처리 방식 경험
  * 토큰 기반 비용 구조의 중요성 이해
  * `.env` 기반 환경변수 관리의 필요성 인지
  * 단순 API 호출을 넘어 대화 기록 설계가 중요하다는 점을 학습

* 다음 주 준비할 것:

  * Chunking: PDF/Markdown 로드 및 텍스트 분할 전략 학습
  * 도메인 데이터를 의미 단위로 분할하는 방법 연구
  * 고정 길이 분할 vs 의미 기반 분할 방식 비교
  * Overlap 전략 이해
  * RAG 구조(임베딩 → 벡터 저장 → 검색 → LLM 입력 흐름) 사전 학습
