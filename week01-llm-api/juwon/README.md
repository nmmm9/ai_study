# 1주차: LLM API 연동 - juwon

## 기술 스택
| 항목 | 선택 | 대안 | 선택 이유 |
|------|------|------|----------|
| LLM API | OpenAI GPT-4o-mini | Claude, Gemini | 비용 효율적이며 API 사용이 간편함 |
| 언어 | Python | JavaScript, Go | LLM API 연동에 가장 널리 사용되는 언어 |
| 환경 관리 | python-dotenv | 직접 환경변수 설정 | API 키 보안 관리 용이 |

## 핵심 구현
- **주요 로직**: OpenAI GPT-4o-mini를 활용한 1:1 대화 터미널 챗봇
  - **Streaming 응답**: 실시간으로 AI 답변 출력
  - **토큰 관리**: 입력/출력 토큰 사용량 추정 및 누적 표시
  - **대화 히스토리 관리**: Sliding Window 방식으로 최근 20개 메시지만 유지
  - **글자 수 제한**: 대화 히스토리가 8000자를 초과하면 오래된 메시지 자동 제거

- **코드 실행 방법**:
```bash
# 1. 의존성 설치
pip install -r requirements.txt

# 2. 챗봇 실행
python chat.py
```

## WHY (의사결정 기록)

### 1. **Q**: 왜 Streaming 방식을 선택했는가?
   **A**:
   - 사용자 경험 향상: 전체 응답을 기다리지 않고 실시간으로 확인 가능
   - 긴 응답의 경우 체감 대기 시간 단축

### 2. **Q**: 왜 토큰 사용량을 추정 방식으로 계산하는가?
   **A**:
   - OpenAI의 streaming API는 `usage` 정보를 반환하지 않음
   - 근사치로 (글자 수 × 1.5)를 사용하여 한국어 토큰 수 추정
   - 정확한 계산을 위해서는 `tiktoken` 라이브러리 필요하지만, 추정치로도 충분

### 3. **Q**: 왜 Sliding Window 방식으로 대화 히스토리를 관리하는가?
   **A**:
   - 무한정 대화 히스토리를 유지하면 토큰 비용이 증가하고 컨텍스트 윈도우 초과 가능
   - 최근 20개 메시지 + 8000자 제한으로 합리적인 비용과 성능 균형 유지

### 4. **Q**: 다르게 구현한다면 어떻게 했을까?
   **A**:
   - **정확한 토큰 계산**: `tiktoken` 라이브러리로 정확한 토큰 수 계산
   - **대화 요약**: 오래된 대화를 제거하는 대신 요약하여 장기 컨텍스트 유지
   - **멀티모달**: 이미지나 파일을 입력으로 받아 처리

## 프로젝트 구조
```
juwon/
├── .env                        # API 키 설정
├── requirements.txt            # 의존성 패키지
├── chat.py                     # 챗봇 메인 코드
├── document_processor.py       # (추후 RAG 구현 시 사용)
├── rag_chat.py                 # (추후 RAG 구현 시 사용)
├── 스터디_1주차_rag의_개념.pdf  # RAG 학습 자료
└── README.md                   # 본 문서
```

## 주요 기능

### 명령어
- `quit`: 프로그램 종료 및 토큰 사용량 출력
- `reset`: 대화 히스토리 초기화
- `usage`: 현재까지 누적 토큰 사용량 확인

### 대화 흐름
1. 사용자 입력 받기
2. 대화 히스토리에 추가
3. 히스토리 정리 (Sliding Window)
4. GPT API 호출 (Streaming)
5. 실시간 응답 출력
6. 토큰 사용량 추정 및 표시

## 트러블슈팅 로그
| # | 문제 상황 | 에러 메시지 | 원인 (Root Cause) | 해결 방법 |
|---|----------|-----------|-------------------|----------|
| - | (향후 추가 예정) | - | - | - |

## 회고
- **이번 주 배운 점**:
  - OpenAI API의 기본 사용법
  - Streaming 방식 응답 처리
  - 토큰 관리와 비용 최적화의 중요성
  - 대화 히스토리 관리 전략

- **다음 주 준비할 것**:
  - RAG 시스템 구현 (문서 기반 질의응답)
  - 정확한 토큰 계산 (tiktoken)
  - 대화 요약 기능 추가
