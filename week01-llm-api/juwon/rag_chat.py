"""
1ì£¼ì°¨ ê³¼ì œ: OpenAI API ì—°ë™ - RAG í„°ë¯¸ë„ ì±—ë´‡
RAG (Retrieval-Augmented Generation)ë¥¼ í™œìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
"""

import os
from dotenv import load_dotenv
from openai import OpenAI
from document_processor import DocumentProcessor

load_dotenv()

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL = "gpt-4o-mini"
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(ë¬¸ì„œ ë‚´ìš©)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.
4. ê°€ëŠ¥í•œ ê²½ìš° ë¬¸ì„œì˜ ì›ë¬¸ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤."""

EMBEDDING_CACHE_FILE = "embeddings_cache.pkl"
PDF_FILE = "ìŠ¤í„°ë””_1ì£¼ì°¨_ragì˜_ê°œë….pdf"

# â”€â”€ ìƒíƒœ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = OpenAI()
doc_processor = DocumentProcessor(client)
conversation: list[dict] = []
total_input_tokens = 0
total_output_tokens = 0


def initialize_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë¬¸ì„œ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±)"""
    global doc_processor

    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    # ìºì‹œëœ ì„ë² ë”©ì´ ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(EMBEDDING_CACHE_FILE):
        print("ğŸ“¦ ìºì‹œëœ ì„ë² ë”© ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...\n")
        doc_processor.load_embeddings(EMBEDDING_CACHE_FILE)
    else:
        # ì—†ìœ¼ë©´ PDF ë¡œë“œí•˜ê³  ì„ë² ë”© ìƒì„±
        if not os.path.exists(PDF_FILE):
            print(f"âŒ ì˜¤ë¥˜: PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_FILE}")
            print("í˜„ì¬ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì„ ë°°ì¹˜í•´ì£¼ì„¸ìš”.\n")
            return False

        doc_processor.load_pdf(PDF_FILE, chunk_size=500)
        doc_processor.create_embeddings()
        doc_processor.save_embeddings(EMBEDDING_CACHE_FILE)

    print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\n")
    return True


def chat_with_rag(user_input: str) -> str:
    """RAG ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬"""
    global total_input_tokens, total_output_tokens

    # 1. ê´€ë ¨ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰
    print("\nğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    similar_chunks = doc_processor.search_similar_chunks(user_input, top_k=3)

    # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“š ì°¾ì€ ê´€ë ¨ ë¬¸ì„œ:")
    for i, (chunk, score) in enumerate(similar_chunks, 1):
        preview = chunk[:100] + "..." if len(chunk) > 100 else chunk
        print(f"  [{i}] (ìœ ì‚¬ë„: {score:.3f}) {preview}")

    # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "\n\n".join([f"[ë¬¸ì„œ {i}]\n{chunk}"
                           for i, (chunk, _) in enumerate(similar_chunks, 1)])

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""ë‹¤ìŒì€ ì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

    # 4. GPT API í˜¸ì¶œ
    conversation.append({"role": "user", "content": prompt})

    print("\nğŸ’­ AI ë‹µë³€ ìƒì„± ì¤‘...\n")
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            *conversation
        ],
        temperature=0.7,
        stream=True
    )

    # 5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
    full_response = ""
    print("[AI] ", end="", flush=True)
    for chunk in response:
        if chunk.choices[0].delta.content:
            text = chunk.choices[0].delta.content
            print(text, end="", flush=True)
            full_response += text

    print("\n")

    # 6. ëŒ€í™” ê¸°ë¡ ì €ì¥
    conversation.append({"role": "assistant", "content": full_response})

    # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì • (ì •í™•í•œ ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”)
    input_tokens = len(prompt.split()) * 1.3
    output_tokens = len(full_response.split()) * 1.3
    total_input_tokens += int(input_tokens)
    total_output_tokens += int(output_tokens)

    print(f"ğŸ’¡ [ì˜ˆìƒ í† í°: ì…ë ¥ ~{int(input_tokens)} / ì¶œë ¥ ~{int(output_tokens)}]\n")

    return full_response


def show_usage():
    """í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥"""
    print("\nâ”€â”€ í† í° ì‚¬ìš©ëŸ‰ (ì¶”ì •ì¹˜) â”€â”€")
    print(f"  ëˆ„ì  ì…ë ¥: ~{total_input_tokens} í† í°")
    print(f"  ëˆ„ì  ì¶œë ¥: ~{total_output_tokens} í† í°")
    print(f"  ì´ í•©ê³„:   ~{total_input_tokens + total_output_tokens} í† í°")
    print(f"  ëŒ€í™” ë©”ì‹œì§€ ìˆ˜: {len(conversation)//2}ê°œ\n")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if not initialize_rag_system():
        return

    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     RAG ê¸°ë°˜ ë¬¸ì„œ ì§ˆì˜ì‘ë‹µ ì±—ë´‡             â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print("â•‘  ëª…ë ¹ì–´:                                     â•‘")
    print("â•‘    quit   - ì¢…ë£Œ                             â•‘")
    print("â•‘    reset  - ëŒ€í™” ì´ˆê¸°í™”                      â•‘")
    print("â•‘    usage  - í† í° ì‚¬ìš©ëŸ‰ í™•ì¸                 â•‘")
    print("â•‘    reload - ë¬¸ì„œ ì¬ë¡œë“œ                      â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    while True:
        try:
            user_input = input("ğŸ’¬ [ì§ˆë¬¸] ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            show_usage()
            break

        if not user_input:
            continue

        if user_input.lower() == "quit":
            show_usage()
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if user_input.lower() == "reset":
            conversation.clear()
            print("âœ… ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
            continue

        if user_input.lower() == "usage":
            show_usage()
            continue

        if user_input.lower() == "reload":
            print("\nğŸ“‚ ë¬¸ì„œë¥¼ ì¬ë¡œë“œí•©ë‹ˆë‹¤...\n")
            if os.path.exists(EMBEDDING_CACHE_FILE):
                os.remove(EMBEDDING_CACHE_FILE)
            initialize_rag_system()
            continue

        # RAG ê¸°ë°˜ ëŒ€í™” ì‹¤í–‰
        chat_with_rag(user_input)


if __name__ == "__main__":
    main()
